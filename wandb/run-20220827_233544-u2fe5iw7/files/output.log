
2022-08-27 23:35:47.998195: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
Metal device set to: Apple M1
systemMemory: 8.00 GB
maxCacheSize: 2.67 GB
2022-08-27 23:35:49,824	INFO services.py:1456 -- View the Ray dashboard at [32m[1mhttp://127.0.0.1:8265
[36m(pid=86338)[39m /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/jax/_src/lib/__init__.py:33: UserWarning: JAX on Mac ARM machines is experimental and minimally tested. Please see https://github.com/google/jax/issues/5501 in the event of problems.
[36m(pid=86338)[39m   warnings.warn("JAX on Mac ARM machines is experimental and minimally tested. "
  0%|                                                                                                                                                                      | 105/262050 [00:01<1:13:13, 59.62it/s]
41
Episode: 100 Mean Reward: -0.010356436135468654
Accuracy: 1.0
New States: 1114 (91.6872427983539%)
Q-table size: 1115
Episode: 200 Mean Reward: -0.010440000495873392
Accuracy: 1.0
New States: 1062 (91.08061749571183%)


  0%|▏                                                                                                                                                                     | 284/262050 [00:05<1:51:39, 39.07it/s]
Episode: 300 Mean Reward: -0.009690000460250304
Accuracy: 1.0
New States: 970 (89.98144712430427%)

  0%|▏                                                                                                                                                                     | 362/262050 [00:07<1:49:27, 39.85it/s]
Episode: 400 Mean Reward: -0.00975000046310015
Accuracy: 1.0
New States: 972 (90.0%)

  0%|▎                                                                                                                                                                     | 443/262050 [00:09<1:53:37, 38.37it/s]
Episode: 500 Mean Reward: -0.00853000040515326
Accuracy: 1.0
New States: 847 (88.59832635983264%)


  0%|▍                                                                                                                                                                     | 595/262050 [00:13<1:55:19, 37.79it/s]
Episode: 600 Mean Reward: -0.009140000434126706
Accuracy: 1.0
New States: 897 (88.3743842364532%)

  0%|▍                                                                                                                                                                     | 671/262050 [00:16<1:56:00, 37.55it/s]
Episode: 700 Mean Reward: -0.009250000439351425
Accuracy: 1.0
New States: 906 (88.39024390243902%)

  0%|▍                                                                                                                                                                     | 747/262050 [00:18<1:58:43, 36.68it/s]
Episode: 800 Mean Reward: -0.007930000376654789
Accuracy: 1.0
New States: 773 (86.36871508379889%)


  0%|▌                                                                                                                                                                     | 895/262050 [00:22<2:01:53, 35.71it/s]
Episode: 900 Mean Reward: -0.007980000379029661
Accuracy: 1.0
New States: 777 (86.52561247216036%)

  0%|▌                                                                                                                                                                     | 967/262050 [00:24<2:00:36, 36.08it/s]
Episode: 1000 Mean Reward: -0.008540000405628235
Accuracy: 1.0
New States: 821 (86.0587002096436%)


  0%|▋                                                                                                                                                                    | 1103/262050 [00:28<2:27:01, 29.58it/s]
Episode: 1100 Mean Reward: -0.008570000407053158
Accuracy: 1.0
New States: 828 (86.43006263048017%)

  0%|▋                                                                                                                                                                    | 1150/262050 [00:29<2:11:32, 33.06it/s]
Episode: 1200 Mean Reward: -0.007230000343406573
Accuracy: 1.0
New States: 703 (85.41919805589308%)


  0%|▊                                                                                                                                                                    | 1290/262050 [00:33<2:06:19, 34.40it/s]
Episode: 1300 Mean Reward: -0.00771000036620535
Accuracy: 1.0
New States: 738 (84.73019517795638%)

  1%|▊                                                                                                                                                                    | 1354/262050 [00:35<2:24:31, 30.06it/s]
Episode: 1400 Mean Reward: -0.007630000362405553
Accuracy: 1.0
New States: 725 (84.00926998841251%)


  1%|▉                                                                                                                                                                    | 1494/262050 [00:40<2:04:37, 34.85it/s]
Episode: 1500 Mean Reward: -0.0075100003567058595
Accuracy: 1.0
New States: 721 (84.72385428907168%)

  1%|▉                                                                                                                                                                    | 1562/262050 [00:42<2:05:47, 34.51it/s]
Episode: 1600 Mean Reward: -0.006880000326782465
Accuracy: 1.0
New States: 648 (82.23350253807106%)


  1%|█                                                                                                                                                                    | 1702/262050 [00:46<2:11:29, 33.00it/s]
Episode: 1700 Mean Reward: -0.0066400003153830765
Accuracy: 1.0
New States: 626 (81.93717277486911%)

  1%|█                                                                                                                                                                    | 1770/262050 [00:48<2:15:57, 31.91it/s]
Episode: 1800 Mean Reward: -0.0061600002925843
Accuracy: 1.0
New States: 576 (80.44692737430168%)

  1%|█▏                                                                                                                                                                   | 1834/262050 [00:50<2:12:14, 32.80it/s]
Episode: 1900 Mean Reward: -0.006240000296384096
Accuracy: 1.0
New States: 582 (80.27586206896552%)


  1%|█▏                                                                                                                                                                   | 1974/262050 [00:54<2:06:15, 34.33it/s]
Episode: 2000 Mean Reward: -0.0059800002840347585
Accuracy: 1.0
New States: 555 (79.51289398280802%)

  1%|█▎                                                                                                                                                                   | 2042/262050 [00:56<2:08:06, 33.83it/s]
Episode: 2100 Mean Reward: -0.005880000279285013
Accuracy: 1.0
New States: 543 (78.92441860465117%)


  1%|█▎                                                                                                                                                                   | 2178/262050 [01:00<2:10:12, 33.26it/s]
Episode: 2200 Mean Reward: -0.006180000293534249
Accuracy: 1.0
New States: 564 (78.55153203342618%)

  1%|█▍                                                                                                                                                                   | 2246/262050 [01:02<2:07:13, 34.03it/s]
Episode: 2300 Mean Reward: -0.005730000272160396
Accuracy: 1.0
New States: 530 (78.7518573551263%)


  1%|█▍                                                                                                                                                                   | 2366/262050 [01:05<2:07:57, 33.83it/s]
Episode: 2400 Mean Reward: -0.0058700002788100395
Accuracy: 1.0
New States: 545 (79.33042212518195%)

  1%|█▌                                                                                                                                                                   | 2434/262050 [01:08<2:09:28, 33.42it/s]
Episode: 2500 Mean Reward: -0.0056500002683606
Accuracy: 1.0
New States: 509 (76.54135338345864%)


  1%|█▌                                                                                                                                                                   | 2570/262050 [01:12<2:08:14, 33.72it/s]
Episode: 2600 Mean Reward: -0.004890000232262537
Accuracy: 1.0
New States: 442 (75.04244482173175%)

  1%|█▋                                                                                                                                                                   | 2638/262050 [01:14<2:07:27, 33.92it/s]
Episode: 2700 Mean Reward: -0.0068100003234576436
Accuracy: 1.0
New States: 621 (79.5134443021767%)


  1%|█▋                                                                                                                                                                   | 2774/262050 [01:18<2:09:38, 33.33it/s]
Episode: 2800 Mean Reward: -0.00503000023891218
Accuracy: 1.0
New States: 447 (74.12935323383084%)

  1%|█▊                                                                                                                                                                   | 2838/262050 [01:20<2:10:12, 33.18it/s]
Episode: 2900 Mean Reward: -0.0055200002621859316
Accuracy: 1.0
New States: 497 (76.22699386503068%)


  1%|█▊                                                                                                                                                                   | 2974/262050 [01:24<2:11:26, 32.85it/s]
Episode: 3000 Mean Reward: -0.00554000026313588
Accuracy: 1.0
New States: 496 (75.84097859327217%)

  1%|█▉                                                                                                                                                                   | 3042/262050 [01:26<2:16:20, 31.66it/s]
Episode: 3100 Mean Reward: -0.005360000254586339
Accuracy: 1.0
New States: 483 (75.94339622641509%)


  1%|██                                                                                                                                                                   | 3178/262050 [01:30<2:10:19, 33.11it/s]
Episode: 3200 Mean Reward: -0.005560000264085829
Accuracy: 1.0
New States: 504 (76.82926829268293%)

  1%|██                                                                                                                                                                   | 3242/262050 [01:32<2:12:36, 32.53it/s]
Episode: 3300 Mean Reward: -0.005380000255536288
Accuracy: 1.0
New States: 470 (73.66771159874608%)


  1%|██                                                                                                                                                                   | 3374/262050 [01:36<2:15:07, 31.91it/s]
Episode: 3400 Mean Reward: -0.005200000246986747
Accuracy: 1.0
New States: 464 (74.83870967741936%)
  1%|██▏                                                                                                                                                                  | 3435/262050 [01:38<2:03:34, 34.88it/s]
Traceback (most recent call last):
  File "/Users/keanl/Desktop/Computer_Science/Research/Q-Table-Learning/Q-table-replay.py", line 245, in <module>
    main()
  File "/Users/keanl/Desktop/Computer_Science/Research/Q-Table-Learning/Q-table-replay.py", line 231, in main
    qtable.train()
  File "/Users/keanl/Desktop/Computer_Science/Research/Q-Table-Learning/Q-table-replay.py", line 150, in train
    self.replay()
  File "/Users/keanl/Desktop/Computer_Science/Research/Q-Table-Learning/Q-table-replay.py", line 97, in replay
    self.q_table[state][action] += self.lr * (reward + self.y * np.max(self.q_table[next_state]) - self.q_table[state][action])
  File "<__array_function__ internals>", line 180, in amax
  File "/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/numpy/core/fromnumeric.py", line 2675, in amax
    @array_function_dispatch(_amax_dispatcher)
KeyboardInterrupt
Traceback (most recent call last):
  File "/Users/keanl/Desktop/Computer_Science/Research/Q-Table-Learning/Q-table-replay.py", line 245, in <module>
    main()
  File "/Users/keanl/Desktop/Computer_Science/Research/Q-Table-Learning/Q-table-replay.py", line 231, in main
    qtable.train()
  File "/Users/keanl/Desktop/Computer_Science/Research/Q-Table-Learning/Q-table-replay.py", line 150, in train
    self.replay()
  File "/Users/keanl/Desktop/Computer_Science/Research/Q-Table-Learning/Q-table-replay.py", line 97, in replay
    self.q_table[state][action] += self.lr * (reward + self.y * np.max(self.q_table[next_state]) - self.q_table[state][action])
  File "<__array_function__ internals>", line 180, in amax
  File "/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/numpy/core/fromnumeric.py", line 2675, in amax
    @array_function_dispatch(_amax_dispatcher)
